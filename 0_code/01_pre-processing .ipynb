{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mainly form the python file: Preprocessing_Germany_DAAD_4.ipynb (combined with the work Preprocessing_Germany_DAAD_2.ipynb )\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "# loading the bibliometric data\n",
    "\n",
    "\n",
    "papers = pd.read_csv('N:\\Bibliometric_Germany\\KB data\\scprp20_Ger_2_this_time_really_GER.csv',encoding=encoding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AUTHOR_ID', 'AFID', 'PK_INTITUTIONS', 'PK_ITEMS', 'PK_AUTHORS',\n",
       "       'PUBYEAR', 'SOURCETITLE', 'DOI', 'source_id', 'PK_SOURCES',\n",
       "       'ARTICLE_TITLE_EN', 'AUTHOR_ID_1', 'LAST_NAME', 'FIRST_NAME',\n",
       "       'INDEXED_NAME', 'INITIALS', 'ORCID_ID', 'INSTITUTION_FULL',\n",
       "       'POSTALCODE', 'CITY', 'COUNTRYCODE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers[\"COUNTRY_CODE\"]=papers[\"COUNTRY_CODE\"].apply(str)\n",
    "papers[\"COUNTRY_CODE\"]=papers[\"COUNTRY_CODE\"].map(lambda x:x.upper())\n",
    "\n",
    "# modify the wrong country code \n",
    "def substitue(x):\n",
    "    if (x==\"YUG\") | (x==\"YUX\"): \n",
    "        x=\"SRB\"\n",
    "    elif (x==\"ROM\"):\n",
    "        x=\"ROU\"\n",
    "    elif (x==\"ANT\"):\n",
    "        x=\"NLD\"\n",
    "    elif (x==\"DDR\"):\n",
    "        x=\"DEU\"\n",
    "    elif (x==\"UMI\"):\n",
    "        x=\"USA\"\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return x\n",
    "\n",
    "papers_1[\"COUNTRYCODE\"]=papers_1[\"COUNTRYCODE\"].map(lambda x: substitue(x))\n",
    "papers_2[\"COUNTRYCODE\"]=papers_2[\"COUNTRYCODE\"].map(lambda x: substitue(x))\n",
    "Paper_aff[\"COUNTRY_CODE\"]=Paper_aff[\"COUNTRY_CODE\"].map(lambda x: substitue(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 1. predict missing country "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# records with country / records without country \n",
    "papers_Country=papers[papers['COUNTRYCODE'].notnull()]\n",
    "papers_PredCountry=papers[papers['COUNTRYCODE'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samping records having country \n",
    "\n",
    "train_counrtry6_new=papers_Country.sample(n=1000000)\n",
    "train_counrtry6_new.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_nov\\\\train_country6_new.csv',encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_Country.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_nov\\\\02_papers_Country.csv\",encoding=encoding)\n",
    "papers_PredCountry.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_nov\\\\02_papers_PredCountry.csv\",encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predic country in Country_prediction_NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read predicted records \n",
    "#papers_Country= pd.read_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_Feb\\\\0_papers_Country.csv\",encoding=encoding)\n",
    "papers_predCountry= pd.read_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_Feb\\\\0_predicted_reliable.csv\",encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### 2. suspicious authors & acquire the ultimate disambiguated records\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers['number_of_countries'] = (papers.groupby('AUTHOR_ID')['COUNTRYCODE']).transform('nunique')\n",
    "papers['number_of_publications'] = (papers.groupby('AUTHOR_ID')['PK_ITEMS']).transform('nunique')\n",
    "papers[\"number_of_yearlyPub\"]=papers['number_of_publications']/(papers.groupby('AUTHOR_ID')['PUBYEAR'].transform('nunique'))\n",
    "papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select DEU-realted authors\n",
    "\n",
    "DEU_author=papers[papers[\"COUNTRYCODE\"]==\"DEU\"]\n",
    "papers_1=papers[papers[\"AUTHOR_ID\"].isin(DEU_author['AUTHOR_ID'])]\n",
    "papers=papers_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suspicious authors (> 6 countries or have publictions monthly)\n",
    "\n",
    "suspicious=papers[(papers[\"number_of_countries\"]>6) | \\\n",
    "                          (papers[\"number_of_publications\"]>292)]\n",
    "suspicious.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_Feb\\\\02_suspicious_records.csv',encoding=encoding)\n",
    "\n",
    "non_suspicious=papers[~ papers['AUTHOR_ID'].isin(suspicious['AUTHOR_ID'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disambiguated_1=pd.read_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_Feb\\\\02_suspicious_records_disambiguated_less_clusters_01.csv',encoding=encoding)\n",
    "\n",
    "disambiguated_1.drop(columns={\"Unnamed: 0\",\"index\",\"Unnamed: 0.1\"},inplace=True)\n",
    "disambiguated_1.rename(columns={\"author_id_revised\":\"AUTHOR_ID_NEW\"},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ineffective records with more than 10 clusters after disambiguations \n",
    "\n",
    "disambiguated_1=disambiguated_1[~(disambiguated_1[\"AUTHOR_ID\"].isin(disambiguated_1[disambiguated_1[\"cluster1\"]>10].AUTHOR_ID.tolist()))]\n",
    "\n",
    "disambiguated_1=disambiguated_1[(disambiguated_1[\"PUBYEAR\"]>1995)&(disambiguated_1[\"PUBYEAR\"]<2021)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguated_1['number_of_countries'] = (disambiguated_1.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')\n",
    "disambiguated_1['number_of_publications'] = (disambiguated_1.groupby('AUTHOR_ID_NEW')['PK_ITEMS']).transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the ultimate records\n",
    "\n",
    "disambiguated_1=disambiguated_1[(disambiguated_1[\"number_of_countries\"]<=6)&(disambiguated_1[\"number_of_publications\"]<=292)]\n",
    "\n",
    "all_records=pd.concat([non_suspicious,disambiguated_1])\n",
    "all_records.to_csv(all_records.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\03_all_records_disambiguated.csv',encoding=encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ASJC code for each author\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the recode after disambigationg suspicious Author_ID\n",
    "country_tuples=(all_records[['DOI','CODE','COUNTRYCODE','AUTHOR_ID_NEW','PUBYEAR']])\n",
    "country_tuples['number_of_countries'] = (country_tuples.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_tuples=country_tuples.replace({'CODE': {'multidiscipl': 1000,'1200;':1200,'1300;':1300,'2800;':2800}})\n",
    "country_tuples=country_tuples.astype({'CODE':int})\n",
    "country_tuples[\"CODE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing subject codes to the two left-most digits (for the main categories)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "country_tuples['CODE_1']=country_tuples['CODE'].apply(lambda x: np.floor(x/100))\n",
    "country_tuples[\"CODE_2\"]=country_tuples[\"CODE_1\"]\n",
    "country_tuples=country_tuples[(country_tuples[\"CODE_2\"]!=81)&(country_tuples[\"CODE_2\"]!=261)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code dictionary (two scales)\n",
    "\n",
    "dic_1 = {\n",
    "10: \"Multidisciplinary\",\n",
    "12: \"Social\",\n",
    "14: \"Social\",\n",
    "18: \"Social\",\n",
    "20: \"Social\",\n",
    "32: \"Social\",\n",
    "33: \"Social\",\n",
    "15: \"Physical\",\n",
    "16: \"Physical\",\n",
    "17: \"Physical\",\n",
    "19: \"Physical\",\n",
    "21: \"Physical\",\n",
    "22: \"Physical\",\n",
    "23: \"Physical\",\n",
    "25: \"Physical\",\n",
    "26: \"Physical\",\n",
    "31: \"Physical\",\n",
    "11: \"Life\",\n",
    "13: \"Life\",\n",
    "24: \"Life\",\n",
    "28: \"Life\",\n",
    "30: \"Life\",\n",
    "27: \"Health\",\n",
    "29: \"Health\",\n",
    "34: \"Health\",\n",
    "35: \"Health\",\n",
    "36: \"Health\"\n",
    "     }\n",
    "dic_2 = {\n",
    "10: \"Multidisciplinary\",\n",
    "12: \"Arts&Humanities\",\n",
    "14: \"Business&Management&Accounting\",\n",
    "18: \"DecisionSciences\",\n",
    "20: \"Economics&Econometrics&Finance\",\n",
    "32: \"Psychology\",\n",
    "33: \"SocialSciences\",\n",
    "15: \"ChemicalEngineering\",\n",
    "16: \"Chemistry\",\n",
    "17: \"ComputerScience\",\n",
    "19: \"Earth&PlanetarySciences\",\n",
    "21: \"Energy\",\n",
    "22: \"Engineering\",\n",
    "23: \"EnvironmentalScience\",\n",
    "25: \"MaterialsScience\",\n",
    "26: \"Mathematics\",\n",
    "31: \"Physics&Astronomy\",\n",
    "11: \"Agricultural&BiologicalSciences\",\n",
    "13: \"Biochemistry&Genetics&MolecularBiology\",\n",
    "24: \"Immunology&Microbiology\",\n",
    "28: \"Neuroscience\",\n",
    "30: \"Pharmacology&Toxicology&Pharmaceutics\",\n",
    "27: \"Medicine\",\n",
    "29: \"Nursing\",\n",
    "34: \"veterinary\",\n",
    "35: \"Dentistry\",\n",
    "36: \"HealthProfessions\"\n",
    "     }\n",
    "\n",
    "country_tuples=country_tuples.replace({\"CODE_1\": dic_1})\n",
    "country_tuples=country_tuples.replace({\"CODE_2\": dic_2})\n",
    "country_tuples.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for finer scale (dic_2)\n",
    "\n",
    "percentage=(country_tuples.groupby('AUTHOR_ID_NEW')['CODE_2'].value_counts(normalize=True).to_frame())\n",
    "percentage=percentage.rename(columns={\"CODE_2\": \"FREQ\"})\n",
    "percentage=percentage.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=percentage[\"AUTHOR_ID_NEW\"].nunique()\n",
    "freq_mean={}\n",
    "freq_std={}\n",
    "codearray=percentage[\"CODE_2\"].unique()\n",
    "codelen=percentage[\"CODE_2\"].nunique()\n",
    "for i in range(0,codelen):\n",
    "    index=codearray[i]\n",
    "    mean=sum((percentage[percentage[\"CODE_2\"]==index])[\"FREQ\"])/num\n",
    "    std=sum((percentage[percentage[\"CODE_2\"]==index])[\"FREQ\"].apply(lambda x: (x-mean)*(x-mean)))/num\n",
    "    #mean=(percentage[percentage[\"CODE_2\"]==index])[\"FREQ\"].mean()\n",
    "    #std=(percentage[percentage[\"CODE_2\"]==index])[\"FREQ\"].std()\n",
    "    freq_mean[index]=mean\n",
    "    freq_std[index]=std**0.5\n",
    "    #freq_std[index]=std\n",
    "print(freq_mean)\n",
    "print(freq_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha is chosen by trial and error such that the fraction of authors for whome disciplines are inferred \n",
    "# would be above 89%\n",
    "\n",
    "alpha=0.25\n",
    "\n",
    "index=0\n",
    "author_discipline={}\n",
    "for identity, row in percentage.groupby(\"AUTHOR_ID_NEW\"):\n",
    "    #print(identity)  \n",
    "    index=row[\"FREQ\"].index[0]\n",
    "    code=row[\"CODE_2\"][index]\n",
    "    significant_index=0\n",
    "    significant_score=0\n",
    "    for i in range(len(row)):\n",
    "        z_score=(list(row[\"FREQ\"])[i]-freq_mean[list(row[\"CODE_2\"])[i]])/(freq_std[list(row[\"CODE_2\"])[i]])\n",
    "        if z_score > alpha:\n",
    "            if z_score>significant_score:\n",
    "                significant_index=i\n",
    "                significant_score=z_score\n",
    "    if significant_score>alpha:\n",
    "        author_discipline[identity]= list(row[\"CODE_2\"])[significant_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"mapping\" the disctionary back into the dataframe to create the column \"MAIN_CODE\"\n",
    "\n",
    "country_tuples_code['MAIN_CODE'] = country_tuples_code['AUTHOR_ID_NEW'].map(author_discipline)\n",
    "\n",
    "# add Multidis for missing values of MAIN_CODE\n",
    "country_tuples_code[\"MAIN_CODE\"].fillna(\"Multidisciplinary\", inplace=True)\n",
    "\n",
    "# select the valid records again, filter invaild records\n",
    "\n",
    "tuples=country_tuples.copy()\n",
    "country_tuples_unique=country_tuples.drop_duplicates(['AUTHOR_ID_NEW','PK_ITEMS']).sort_values(by=['AUTHOR_ID_NEW','PUBYEAR'])\n",
    "country_tuples_unique['number_of_countries'] = (country_tuples_unique.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')\n",
    "country_tuples_unique[\"PUBLICATION_COUNT\"] = (country_tuples_unique.groupby('AUTHOR_ID_NEW')['PK_ITEMS']).transform('nunique')\n",
    "\n",
    "country_tuples_unique=country_tuples_unique[country_tuples_unique[\"number_of_countries\"]<5]\n",
    "country_tuples_unique=country_tuples_unique[country_tuples_unique[\"PUBLICATION_COUNT\"]<293]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the authors including movers and non-movers\n",
    "country_tuples_code.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\05_AuthorID_Main_Code_old_method.csv\",encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. NLP for main disciplines over publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load key words\n",
    "Key_word= pd.read_csv(\"N:\\Bibliometrics\\data\\scopus_raw_2020\\scopusrp20_utf8_keywords.csv\",encoding=encoding)\n",
    "Key_word['KEYWORD']=Key_word['KEYWORD'].astype(\"str\")\n",
    "Key_word.drop(columns={\"KEYWORD_TYPE\",\"PK_KEYWORDS\"},inplace=True)\n",
    "\n",
    "Key_word.rename(columns={'FK_ITEMS':'PK_ITEMS'}, inplace=True)\n",
    "Key_word_new=Key_word.groupby(by='PK_ITEMS').apply(lambda x:' '.join(x['KEYWORD'])) \n",
    "Key_word_new=Key_word_new.to_frame().reset_index()\n",
    "Key_word_new.rename(columns={0:'KEYWORD'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing for LDA\n",
    "# text source: Title + Journal title + Keyword \n",
    "\n",
    "all_records['ARTICLE_TITLE_EN']=all_records['ARTICLE_TITLE_EN'].fillna(\" \")\n",
    "all_records['SOURCETITLE']=all_records['SOURCETITLE'].fillna(\"\")\n",
    "all_records['KEYWORD']=all_records['KEYWORD'].fillna(\" \")\n",
    "all_records[\"Text\"]= all_records['ARTICLE_TITLE_EN']+\" \"+all_records['SOURCETITLE']+\" \"+all_records['KEYWORD']\n",
    "\n",
    "# keep only text \n",
    "author_text=all_records[[\"AUTHOR_ID_NEW\",\"Text\"]]\n",
    "author_text.drop_duplicates(inplace=True)\n",
    "author_text=author_text.groupby(by='AUTHOR_ID_NEW').apply(lambda x:' '.join(x['Text']))\n",
    "author_text=author_text.to_frame().reset_index()\n",
    "author_text.rename(columns={0:'Text'},inplace=True)\n",
    "\n",
    "#save to file\n",
    "author_text.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\04_all_records_Text.csv',encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Citation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load citation data\n",
    "\n",
    "citations=pd.read_csv(\"N:\\\\Theile\\\\bibliometry\\\\scopus_rp_2020\\\\scopus_rp_2020_all_citingcount.csv\",encoding=encoding)\n",
    "citations=citations.rename(columns={\"RP2020PK_ITEMS\":\"PK_ITEMS\"})\n",
    "citations=citations.drop(columns={\"UT_EID\"})\n",
    "publications=pd.merge(country_tuples_unique,citations,how='left',on=\"PK_ITEMS\")\n",
    "publications=publications.drop_duplicates(['AUTHOR_ID_NEW','PK_ITEMS']).sort_values(by=['AUTHOR_ID_NEW','PUBYEAR'])\n",
    "\n",
    "\n",
    "all_records_2['TOTAL_CITATIONS']=all_records_2.groupby(['AUTHOR_ID_NEW'])['COUNT'].transform('sum')\n",
    "all_records_2[\"AVERAGE_CITATIIONS_perpub\"]=(all_records_2[\"TOTAL_CITATIONS\"]/all_records_2[\"PUBLICATION_COUNT\"])\n",
    "\n",
    "all_records_2['min_year'] = (all_records_2.groupby('AUTHOR_ID_NEW')['PUBYEAR']).transform('min')\n",
    "all_records_2['ACADEMIC_AGE'] = (2020-all_records_2['min_year']+1)\n",
    "all_records_2=all_records_2.drop('min_year', axis=1)\n",
    "\n",
    "all_records_2[\"AVERAGE_CITATIIONS_peryear\"]=(all_records_2[\"TOTAL_CITATIONS\"]/all_records_2[\"ACADEMIC_AGE\"])\n",
    "all_records_2=all_records_2.rename(columns={\"COUNT\":\"CITATION\"})\n",
    "\n",
    "all_records_2.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\06_AuthorID_Citation_Age.csv\",encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Migration detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_tuples['min_year'] = (country_tuples.groupby('AUTHOR_ID_NEW')['PUBYEAR']).transform('min')\n",
    "country_tuples['number_of_countries'] = (country_tuples.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')\n",
    "country_tuples[\"PUBLICATION_COUNT\"] = (country_tuples.groupby('AUTHOR_ID_NEW')['PK_ITEMS']).transform('nunique')\n",
    "country_tuples['ACADEMIC_AGE'] = (2020-country_tuples['min_year']+1)\n",
    "country_tuples=country_tuples.drop('min_year', axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_tuples[\"PUBYEAR_sort\"]=country_tuples.groupby(\"AUTHOR_ID_NEW\")[\"PUBYEAR\"].rank(ascending=1,method='dense')\n",
    "country_tuples[\"PUBYEAR_sort_rev\"]=country_tuples.groupby(\"AUTHOR_ID_NEW\")[\"PUBYEAR\"].rank(ascending=0,method='dense')\n",
    "country_tuples=country_tuples.sort_values(by=['AUTHOR_ID_NEW','PUBYEAR',\"PK_ITEMS\"])\n",
    "country_tuples['No_Country_perpub'] = (country_tuples.groupby(['AUTHOR_ID_NEW',\"PK_ITEMS\"])['COUNTRYCODE']).transform('nunique')\n",
    "country_tuples['No_Country_peryear'] = (country_tuples.groupby(['AUTHOR_ID_NEW',\"PUBYEAR\"])['COUNTRYCODE']).transform('nunique')\n",
    "\n",
    "country_tuples=country_tuples.sort_values(by=['AUTHOR_ID_NEW','PUBYEAR',\"PK_ITEMS\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for determining the origin and destination countries if the first year and last year covers multiple countries\n",
    "#\n",
    "# modication version, but origin and destination will still be modifed in the next steps \n",
    "\n",
    "#****************************************************************\n",
    "from scipy import stats\n",
    "\n",
    "country_tuples[\"DESTINATION\"]=\"destination\"\n",
    "country_tuples[\"containDEU\"]=2\n",
    "origin_list={}\n",
    "destination_list={}\n",
    "containDEU={}\n",
    "for identity, row in country_tuples.groupby(\"AUTHOR_ID_NEW\"):\n",
    "    if row['COUNTRYCODE'].str.contains('DEU').any():\n",
    "        DEU=1\n",
    "    else:\n",
    "        DEU=0\n",
    "    \n",
    "    #if (row[\"No_Country_perpub\"].iloc[0]>1) & ((row[\"PUBYEAR_sort\"].iloc[0])!=(row[\"PUBYEAR_sort_rev\"].iloc[0])):\n",
    "    \n",
    "    if (row[\"No_Country_peryear\"].iloc[0]>1):\n",
    "        origin=row[row[\"PUBYEAR_sort\"]==1].COUNTRYCODE.mode().iloc[0]\n",
    "        \n",
    "    else:\n",
    "        origin=row[\"COUNTRYCODE\"].iloc[0]\n",
    "        \n",
    "    #f (row[\"No_Country_perpub\"].iloc[-1]>1) & ((row[\"PUBYEAR_sort\"].iloc[-1])!=(row[\"PUBYEAR_sort_rev\"].iloc[-1])):\n",
    "    if (row[\"No_Country_peryear\"].iloc[-1]>1):\n",
    "        destination=row[row[\"PUBYEAR_sort_rev\"]==1].COUNTRYCODE.mode().iloc[0]\n",
    "    else: \n",
    "        destination=row[\"COUNTRYCODE\"].iloc[-1] \n",
    "        \n",
    "    if (row[\"ACADEMIC_AGE\"].iloc[0]==1):\n",
    "        origin=row.COUNTRYCODE.mode().iloc[0]\n",
    "        destination=row.COUNTRYCODE.mode().iloc[0]\n",
    "    else: \n",
    "        pass          \n",
    "        \n",
    "    \n",
    "    origin_list[identity]= origin\n",
    "    destination_list[identity]= destination\n",
    "    containDEU[identity]= DEU\n",
    "        \n",
    "country_tuples[\"ORIGIN\"]=country_tuples['AUTHOR_ID_NEW'].map(origin_list)\n",
    "country_tuples[\"DESTINATION\"]=country_tuples['AUTHOR_ID_NEW'].map(destination_list)   \n",
    "country_tuples[\"containDEU\"]=country_tuples['AUTHOR_ID_NEW'].map(containDEU) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backup\n",
    "country_tuples_ori=country_tuples.copy()\n",
    "country_tuples=country_tuples[country_tuples[\"containDEU\"]==1]\n",
    "country_tuples.drop(columns={\"No_Country_perpub\",\"containDEU\"},inplace=True)\n",
    "\n",
    "country_tuples['number_of_countries'] = (country_tuples.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')\n",
    "country_tuples[\"PUBLICATION_COUNT\"] = (country_tuples.groupby('AUTHOR_ID_NEW')['PK_ITEMS']).transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-movers: \n",
    "non_movers=country_tuples[(country_tuples[\"number_of_countries\"]==1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode country for each year\n",
    "\n",
    "# mode for countrycode\n",
    "locations_by_year=country_tuples[['AUTHOR_ID_NEW','PUBYEAR','COUNTRYCODE',\"ORIGIN\",\"DESTINATION\"]].groupby(['AUTHOR_ID_NEW','PUBYEAR'])['COUNTRYCODE'].agg(pd.Series.mode).to_frame()\n",
    "\n",
    "# sum for countrycode\n",
    "#locations_by_year=population.groupby(['AUTHOR_ID_NEW','PUBYEAR'])['COUNTRYCODE'].agg(pd.Series.sum).to_frame()\n",
    "\n",
    "#Resetting the index\n",
    "locations_by_year=pd.DataFrame(locations_by_year).reset_index()\n",
    "# Tom's functions work if the countrycode is a string as opposed to an array when it is multiple mode\n",
    "locations_by_year=locations_by_year.astype({'COUNTRYCODE':str})\n",
    "locations_by_year.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "locations_by_year[\"AUTHOR_ID_NEW_NEXT\"]=locations_by_year.AUTHOR_ID_NEW.shift(-1)\n",
    "locations_by_year[\"AUTHOR_ID_NEW_LAST\"]=locations_by_year.AUTHOR_ID_NEW.shift(1)\n",
    "#locations_by_year[\"COUNTRYCODE_NEXT\"]=locations_by_year.apply(lambda row:row.COUNTRYCODE.shift(-1) if(row.AUTHOR_ID_NEW==row.AUTHOR_ID_NEW_NEXT) else np.NaN)\n",
    "locations_by_year['COUNTRYCODE_NEXT'] = locations_by_year.COUNTRYCODE.shift(-1)\n",
    "locations_by_year['COUNTRYCODE_LAST'] = locations_by_year.COUNTRYCODE.shift(1)\n",
    "\n",
    "#null value , np.nan\n",
    "locations_by_year.loc[locations_by_year['AUTHOR_ID_NEW'] != locations_by_year['AUTHOR_ID_NEW_NEXT'], 'COUNTRYCODE_NEXT'] = np.NaN\n",
    "locations_by_year.loc[locations_by_year['AUTHOR_ID_NEW'] != locations_by_year['AUTHOR_ID_NEW_LAST'], 'COUNTRYCODE_LAST'] = np.NaN            \n",
    "\n",
    "locations_by_year['number_of_countries_new'] = (locations_by_year.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_records=country_tuples.drop_duplicates()\n",
    "author_records_1=author_records[[\"AUTHOR_ID_NEW\",\"PUBLICATION_COUNT\",\"ACADEMIC_AGE\",'ORIGIN', 'DESTINATION']]\n",
    "author_mergeLocation=pd.merge(locations_by_year,author_records_1,how='left',on=\"AUTHOR_ID_NEW\") \n",
    "#author_mergeLocation.rename(columns={'COUNTRYCODE_x':'COUNTRYCODE',\"COUNTRYCODE_y\":\"locations_by_year\"},inplace=True)                                \n",
    "\n",
    "#author_mergeLocationgeonames_admin1_code\n",
    "#author_mergeLocation[\"number_of_countries\"]=(author_mergeLocation.groupby('AUTHOR_ID_NEW')['COUNTRYCODE']).transform('nunique')\n",
    "\n",
    "author_mergeLocation.drop_duplicates([\"AUTHOR_ID_NEW\",\"PUBYEAR\"],inplace=True)\n",
    "\n",
    "author_mergeLocation.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\07_Location_by_year_useful.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_mergeLocation.rename(columns={\"number_of_countries_new\":\"number_of_countries\"},inplace=True)\n",
    "moving_author=author_mergeLocation[(author_mergeLocation[\"number_of_countries\"]>1) & (author_mergeLocation[\"PUBLICATION_COUNT\"]>1) & (author_mergeLocation[\"ACADEMIC_AGE\"]>1)]\n",
    "non_moving_author=author_mergeLocation[(author_mergeLocation[\"number_of_countries\"]==1) | (author_mergeLocation[\"PUBLICATION_COUNT\"]==1)| (author_mergeLocation[\"ACADEMIC_AGE\"]==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for multiple modes value \n",
    "\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "def mode_list(li):\n",
    "    \n",
    "    mode= [k for k,v in Counter(li).items() if v == max(Counter(li).values())]\n",
    "    \n",
    "    if (len(mode)==1):\n",
    "        mode_str=''.join(mode)\n",
    "    else:\n",
    "        mode_str=mode[randint(0,len(mode)-1)]\n",
    "    \n",
    "    return mode_str\n",
    "bb=['BEL', 'DEU']\n",
    "mode_list(bb)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Set\n",
    "from random import randint\n",
    "import re\n",
    "# Creating country of residence for all records (next chunck)\n",
    "\n",
    "%time\n",
    "#%debug\n",
    "residenceCountry = None\n",
    "ORIGIN_NEW = None\n",
    "lastyear = None\n",
    "\n",
    "newtable = []\n",
    "newrow = []\n",
    "\n",
    "authorid = -1\n",
    "moving_author.sort_values(by=['AUTHOR_ID_NEW','PUBYEAR'],inplace=True)\n",
    "for row in moving_author.itertuples():\n",
    "    #print(row)\n",
    "    newrow = row._asdict()\n",
    "    newrow_1 = row._asdict()\n",
    "    if row.AUTHOR_ID_NEW != authorid:\n",
    "        #print(\"------------------------------------new auhtor!!\")\n",
    "        #new author\n",
    "        residenceCountry = None\n",
    "        ORIGIN_NEW = None\n",
    "        lastyear = None\n",
    "        i=1\n",
    "    authorid = row.AUTHOR_ID_NEW\n",
    "    \n",
    "    #print(residenceCountry)\n",
    "    #first country: take the first mode\n",
    "    # 1. first year--multiple countries \n",
    "    #    1.1 2nd year--1 country \n",
    "    #        1.1.1 overlap with 1st year: remove the overaped countries (??), mode\n",
    "    #        1.1.2 overlap with 1st year: mode\n",
    "    #    1.2 2nd year-- > 1 countries\n",
    "    #        ---union of 1st year and 2nd year\n",
    "    #        1.2.1 union>=1:  mode (union) \n",
    "    #        1.2.2 union>=0:  mode (1st year)\n",
    "    # 2. last year--multiple countries \n",
    "    # combine with last year, mode()    \n",
    "#***************************************************************\n",
    "#new thougts (oct 21)\n",
    "    # 1. first year--multiple countries \n",
    "    #    union with second year\n",
    "    #    1.1 union>=1: mode(union) \n",
    "    #    1.2 union==0: mode(1st year) \n",
    "    # 2. last year--multiple countries \n",
    "    #    union with (last-1) year\n",
    "    #    1.1 union>=1: mode(union) \n",
    "    #    1.2 union==0: mode(last year) \n",
    "\n",
    "\n",
    "    if not residenceCountry:\n",
    "        if ((len(row.COUNTRYCODE)>3) ):\n",
    "            # remove \",\", '[]'\n",
    "            xx=re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE)\n",
    "            if len(row.COUNTRYCODE_NEXT)==3:\n",
    "                \n",
    "                if any(row.COUNTRYCODE_NEXT in s for s in xx):\n",
    "                    xx.remove(row.COUNTRYCODE_NEXT)\n",
    "                    newrow[\"residenceCountry\"] = mode_list(xx)\n",
    "                    residenceCountry = newrow[\"residenceCountry\"]\n",
    "                else: \n",
    "                    newrow[\"residenceCountry\"] = xx[randint(0,len(xx)-1)]\n",
    "                    residenceCountry = newrow[\"residenceCountry\"]\n",
    "                    \n",
    "                ORIGIN_NEW=newrow[\"residenceCountry\"]\n",
    "                newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW   \n",
    "                    \n",
    "            else:\n",
    "                yy=re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE_NEXT)\n",
    "                union=list(set(xx)&(set(yy)))\n",
    "                #yy=list(set(xx)-(set(xx)&set(yy)))\n",
    "                if len(union)>0: \n",
    "                    newrow[\"residenceCountry\"] = union[randint(0,len(union)-1)]\n",
    "                    residenceCountry = newrow[\"residenceCountry\"]\n",
    "                    \n",
    "                else:\n",
    "                    newrow[\"residenceCountry\"] = mode_list(xx)\n",
    "                    residenceCountry = newrow[\"residenceCountry\"]\n",
    "                    \n",
    "                ORIGIN_NEW=newrow[\"residenceCountry\"]\n",
    "                newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW   \n",
    "                \n",
    "        else:\n",
    "            newrow[\"residenceCountry\"]=row.COUNTRYCODE\n",
    "            residenceCountry = newrow[\"residenceCountry\"]\n",
    "            ORIGIN_NEW=newrow[\"residenceCountry\"]\n",
    "            newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW    \n",
    "\n",
    "        \n",
    "    elif row.AUTHOR_ID_NEW != row.AUTHOR_ID_NEW_NEXT:    \n",
    "        if (len(row.COUNTRYCODE)>3):\n",
    "            xx=re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE)\n",
    "            if len(row.COUNTRYCODE_LAST)>3:\n",
    "                xx.extend(re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE_LAST))\n",
    "                newrow[\"residenceCountry\"] = mode_list(xx)\n",
    "            else: \n",
    "                xx.append(row.COUNTRYCODE_LAST)\n",
    "                newrow[\"residenceCountry\"] = mode_list(xx)\n",
    "        else:\n",
    "            newrow[\"residenceCountry\"]=row.COUNTRYCODE \n",
    "                   \n",
    "        residenceCountry = None\n",
    "        DESTINATION_NEW=newrow[\"residenceCountry\"]\n",
    "        newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW\n",
    "        newrow[\"DESTINANTION_NEW\"]=DESTINATION_NEW\n",
    "\n",
    "    elif len(row.COUNTRYCODE)>3:\n",
    "        # there is already a residenceCountry and we have multiple countries\n",
    "        modes = re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE)\n",
    "        \n",
    "        if len(row.COUNTRYCODE_NEXT)>3:\n",
    "            modes.extend(re.findall(\"'([A-Z]+)'\",row.COUNTRYCODE_NEXT))\n",
    "            newrow[\"residenceCountry\"] = mode_list(modes)\n",
    "        else:\n",
    "            modes.append(row.COUNTRYCODE_NEXT)\n",
    "            newrow[\"residenceCountry\"] = mode_list(modes)\n",
    "        \n",
    "        residenceCountry = row.COUNTRYCODE\n",
    "        newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        # we have only one mode -- take that!\n",
    "        #print(\"sdf3- only one mode!\", residenceCountry,row.COUNTRYCODE)\n",
    "        newrow[\"residenceCountry\"] = row.COUNTRYCODE\n",
    "        residenceCountry=row.COUNTRYCODE\n",
    "        newrow[\"ORIGIN_NEW\"]=ORIGIN_NEW\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    #print(\"-------------\",newrow)\n",
    "    newtable.append(newrow)\n",
    "    #newtable.append(newrow_1)\n",
    "    \n",
    "dfcorrected = pd.DataFrame(newtable)\n",
    "dfcorrected['DESTINATION_NEW']=dfcorrected.groupby(['AUTHOR_ID_NEW'])['DESTINANTION_NEW'].apply(lambda x: x.fillna(x.mode()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcorrected.drop(columns={\"COUNTRYCODE_NEXT\",\"AUTHOR_ID_NEW_NEXT\",\"ORIGIN\",\"DESTINATION\",\"COUNTRYCODE_LAST\",\"AUTHOR_ID_NEW_LAST\"},inplace=True)\n",
    "dfcorrected.rename(columns={\"ORIGIN_NEW\":\"ORIGIN\"},inplace=True)\n",
    "dfcorrected.rename(columns={\"DESTINATION_NEW\":\"DESTINATION\"},inplace=True)\n",
    "dfcorrected['number_of_countries'] = (dfcorrected.groupby('AUTHOR_ID_NEW')['residenceCountry']).transform('nunique')\n",
    "dfcorrected['min_year'] = (dfcorrected.groupby('AUTHOR_ID_NEW')['PUBYEAR']).transform('min')\n",
    "\n",
    "dfcorrected.to_csv('N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\07_new_Location_by_year_mover_corrected.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display migration event \n",
    "\n",
    "from math import floor,ceil\n",
    "\n",
    "def get_migrationevents_from_dfcorrected(dfcorrected):\n",
    "    migrationtable = []\n",
    "    inmigration = []\n",
    "    outmigration = [] #(country, year, )\n",
    "    population = []\n",
    "\n",
    "    authorid=-1\n",
    "    lastcountry = None\n",
    "    lastyear= None\n",
    "    thisyearcountry = None\n",
    "    for row in dfcorrected.itertuples():\n",
    "        #print(row)#newrow=\n",
    "        newrow = row._asdict()\n",
    "        year = row.PUBYEAR\n",
    "\n",
    "        if row.AUTHOR_ID_NEW != authorid:\n",
    "            #print(\"------------------------------------new auhtor!!\")\n",
    "            #new author\n",
    "            residenceCountry = None\n",
    "            thisyearcountry = None\n",
    "            lastyear = None\n",
    "            lastcountry=None\n",
    "            authorid = row.AUTHOR_ID_NEW\n",
    "\n",
    "        thisyearcountry = row.residenceCountry\n",
    "        population.append((thisyearcountry, year, 1))\n",
    "        if  (lastcountry != thisyearcountry):  # (pd.notnull(lastcountry)) and\n",
    "            #migration event!\n",
    "            #print(\"Migration-Event!\", lastcountry,thisyearcountry)\n",
    "            if lastcountry:\n",
    "                migrationyear = ceil((lastyear+year)/2)\n",
    "                newrow[\"from\"] = lastcountry \n",
    "                newrow[\"to\"] = thisyearcountry\n",
    "                \n",
    "                newrow[\"move_year\"] = int(ceil((lastyear+year)/2))\n",
    "                inmigration.append((thisyearcountry, migrationyear))\n",
    "                outmigration.append((lastcountry,migrationyear))\n",
    "                newrow[\"migrationevents\"] =  lastcountry+\"|\" + thisyearcountry\n",
    "            else:\n",
    "                newrow[\"migrationevents\"] =  \"nnn|\" + thisyearcountry\n",
    "        lastcountry = row.residenceCountry\n",
    "        lastyear=year\n",
    "        migrationtable.append(newrow)\n",
    "\n",
    "    dfmigrationevents = pd.DataFrame(migrationtable)\n",
    "    dfinmigration = pd.DataFrame(inmigration)\n",
    "    dfoutmigration = pd.DataFrame(outmigration)\n",
    "    dfpopulation = pd.DataFrame(population)\n",
    "    return dfmigrationevents,dfinmigration, dfoutmigration, dfpopulation\n",
    "\n",
    "\n",
    "dfmigrationevents,dfinmigration, dfoutmigration, dfpopulation = get_migrationevents_from_dfcorrected(dfcorrected)\n",
    "dfmigrationevents=dfmigrationevents.drop(columns=['Index', '_1'])\n",
    "\n",
    "dfmigrationevents=dfmigrationevents.groupby(\"AUTHOR_ID_NEW\").filter(lambda g: any(g.residenceCountry == 'DEU')) \n",
    "dfmigrationevents=dfmigrationevents[dfmigrationevents[\"number_of_countries\"]>1]\n",
    "\n",
    "fmigrationevents.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\08_new1_migrationevents_from_to_moveyear_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foe non-movers\n",
    "non_moving_author['min_year'] = (non_moving_author.groupby('AUTHOR_ID_NEW')['PUBYEAR']).transform('min')\n",
    "non_moving_author['max_year'] = (non_moving_author.groupby('AUTHOR_ID_NEW')['PUBYEAR']).transform('max')\n",
    "\n",
    "non_moving_author.to_csv(\"N:\\\\Bibliometric_Germany\\\\KB data\\\\processed\\\\new_2021_April\\\\08_new_non_movers_Germany.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result: different migration types\n",
    "\n",
    "print(dfmigrationevents.AUTHOR_ID_NEW.nunique())\n",
    "print(dfmigrationevents[(dfmigrationevents[\"ORIGIN\"]==\"DEU\")&(dfmigrationevents[\"DESTINATION\"]!=\"DEU\")].AUTHOR_ID_NEW.nunique())\n",
    "print(dfmigrationevents[(dfmigrationevents[\"ORIGIN\"]!=\"DEU\")&(dfmigrationevents[\"DESTINATION\"]==\"DEU\")].AUTHOR_ID_NEW.nunique())\n",
    "print(dfmigrationevents[(dfmigrationevents[\"ORIGIN\"]==\"DEU\")&(dfmigrationevents[\"DESTINATION\"]==\"DEU\")].AUTHOR_ID_NEW.nunique())\n",
    "print(dfmigrationevents[(dfmigrationevents[\"ORIGIN\"]!=\"DEU\")&(dfmigrationevents[\"DESTINATION\"]!=\"DEU\")].AUTHOR_ID_NEW.nunique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
